In this study, beyond implementing the minBERT model to perform semantic analysis, we investigate in various fine-tuning strategies of BERT model to make it simultaneously perform well on multiple downstream tasks, including sentiment analysis, paraphrase detection, and semantic textual similarity. We explore various extensions on BERT including gradient surgery, tuning model architecture and hyperparameters, incorporating additional datasets, and fine-tuning individual tasks. Our model achieved **64%** overall accuracy on the given test set.